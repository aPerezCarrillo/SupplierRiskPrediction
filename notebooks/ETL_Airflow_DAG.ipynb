{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "307b949a-699a-493e-b91b-44508fb93722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67ee24b2-9a8e-4325-87f8-b5af4d1d9db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/alfonzo/Library/Application Support/pipx/venvs/jupyterlab/bin/python'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7106f42-b2bd-4bb9-90b0-abadc22e5da7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting elasticsearch\n",
      "  Downloading elasticsearch-8.17.1-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting elastic-transport<9,>=8.15.1 (from elasticsearch)\n",
      "  Downloading elastic_transport-8.17.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in /Users/alfonzo/Library/Application Support/pipx/venvs/jupyterlab/lib/python3.12/site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2.2.1)\n",
      "Requirement already satisfied: certifi in /Users/alfonzo/Library/Application Support/pipx/venvs/jupyterlab/lib/python3.12/site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2024.2.2)\n",
      "Downloading elasticsearch-8.17.1-py3-none-any.whl (653 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m654.0/654.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading elastic_transport-8.17.0-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.5/64.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: elastic-transport, elasticsearch\n",
      "Successfully installed elastic-transport-8.17.0 elasticsearch-8.17.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/alfonzo/Library/Application Support/pipx/venvs/jupyterlab/bin/python -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!'/Users/alfonzo/Library/Application Support/pipx/venvs/jupyterlab/bin/python' -m pip install elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50ea0ed-d13b-42b8-a26b-b2d1159a39d3",
   "metadata": {},
   "source": [
    "# Airflow DAG code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a58ace3-d4ac-46f7-bc9e-16ba5cb43f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/var/folders/my/vcxzmhpd1l1f6bhyh61p0qqh0000gn/T/ipykernel_53277/2505346129.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">25</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> RemovedInAirflow3Warning</span><span style=\"color: #808000; text-decoration-color: #808000\">: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33m/var/folders/my/vcxzmhpd1l1f6bhyh61p0qqh0000gn/T/ipykernel_53277/\u001b[0m\u001b[1;33m2505346129.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m25\u001b[0m\u001b[1;33m RemovedInAirflow3Warning\u001b[0m\u001b[33m: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Task(PythonOperator): extract_transform_load>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from pydantic import BaseModel\n",
    "import psycopg2\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Default arguments for the DAG\n",
    "default_args = {\n",
    "    \"owner\": \"airflow\",\n",
    "    \"depends_on_past\": False,\n",
    "    \"start_date\": datetime(2024, 2, 12),\n",
    "    \"retries\": 1,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    \"non_compliance_etl\",\n",
    "    default_args=default_args,\n",
    "    description=\"ETL pipeline for non-compliance reports\",\n",
    "    schedule_interval=timedelta(days=1),\n",
    ")\n",
    "\n",
    "# Data extraction functions\n",
    "def fetch_data_from_api():\n",
    "    url = \"https://api.fda.gov/warning-letters\"  # Example API endpoint\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    return []\n",
    "\n",
    "def scrape_web_page(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        return soup.get_text(\" \", strip=True)\n",
    "    return \"\"\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_image(file_path):\n",
    "    image = Image.open(file_path)\n",
    "    return pytesseract.image_to_string(image)\n",
    "\n",
    "# Data transformation functions\n",
    "class WarningLetterData(BaseModel):\n",
    "    supplier_id: str\n",
    "    record_date: str\n",
    "    severity_level: str\n",
    "    category_of_violation: str\n",
    "    root_cause_category: str\n",
    "    corrective_actions_suggested: list\n",
    "    affected_product: str\n",
    "    process_involved: str\n",
    "    tone_of_letter: str\n",
    "    length_of_letter: str\n",
    "    deadline_for_resolution: str\n",
    "    resolution_status: str\n",
    "    follow_up_actions: list\n",
    "\n",
    "def extract_json_from_response(response_text):\n",
    "    json_pattern = r\"\\{.*\\}\"\n",
    "    match = re.search(json_pattern, response_text, re.DOTALL)\n",
    "    return match.group(0) if match else None\n",
    "\n",
    "def structure_data(text):\n",
    "    llm = HuggingFaceHub(repo_id=\"tiiuae/falcon-7b-instruct\", model_kwargs={\"temperature\": 0.7})\n",
    "    prompt = (\n",
    "        \"Extract the following structured JSON from the text: \"\n",
    "        \"['supplier_id', 'record_date', 'severity_level', 'category_of_violation', 'root_cause_category', \"\n",
    "        \"'corrective_actions_suggested', 'affected_product', 'process_involved', 'tone_of_letter', \"\n",
    "        \"'length_of_letter', 'deadline_for_resolution', 'resolution_status', 'follow_up_actions']. \"\n",
    "        \"Text: \" + text\n",
    "    )\n",
    "    response = llm(prompt)\n",
    "    json_data = extract_json_from_response(response)\n",
    "    if json_data:\n",
    "        try:\n",
    "            parsed_data = json.loads(json_data)\n",
    "            return WarningLetterData(**parsed_data).dict()\n",
    "        except (json.JSONDecodeError, ValueError) as e:\n",
    "            return {\"error\": f\"Failed to parse extracted JSON: {str(e)}\"}\n",
    "    else:\n",
    "        return {\"error\": \"No valid JSON found\"}\n",
    "\n",
    "# Data loading functions\n",
    "def load_to_postgres(data):\n",
    "    conn = psycopg2.connect(dbname=\"compliance_db\", user=\"user\", password=\"pass\", host=\"localhost\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO warning_letters (supplier_id, record_date, severity_level, category_of_violation, root_cause_category,\n",
    "        corrective_actions_suggested, affected_product, process_involved, tone_of_letter, length_of_letter,\n",
    "        deadline_for_resolution, resolution_status, follow_up_actions)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\", tuple(data.values()))\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "def load_to_elasticsearch(data):\n",
    "    es = Elasticsearch([{\"host\": \"localhost\", \"port\": 9200}])\n",
    "    es.index(index=\"warning_letters\", body=data)\n",
    "\n",
    "def extract_transform_load():\n",
    "    api_data = fetch_data_from_api()\n",
    "    scraped_data = scrape_web_page(\"https://www.fda.gov/warning-letters\")\n",
    "    \n",
    "    combined_text = \"\\n\".join([scraped_data] + [entry['text'] for entry in api_data if 'text' in entry])\n",
    "    structured_data = structure_data(combined_text)\n",
    "    \n",
    "    if \"error\" not in structured_data:\n",
    "        load_to_postgres(structured_data)\n",
    "        load_to_elasticsearch(structured_data)\n",
    "\n",
    "task_etl = PythonOperator(\n",
    "    task_id=\"extract_transform_load\",\n",
    "    python_callable=extract_transform_load,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "task_etl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082a9e0b-a8ca-43a0-8661-d5b57d204f2a",
   "metadata": {},
   "source": [
    "# Render DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e11fe32-a3ba-4b73-91e7-e7b74db0b4e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.1 (20241206.2353)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"203pt\" height=\"44pt\"\n",
       " viewBox=\"0.00 0.00 202.83 44.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-40 198.83,-40 198.83,4 -4,4\"/>\n",
       "<!-- extract_transform_load -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>extract_transform_load</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"97.41\" cy=\"-18\" rx=\"97.41\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"97.41\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">extract_transform_load</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x122092f60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from airflow.utils.dag_cycle_tester import check_cycle\n",
    "from airflow.models import DAG\n",
    "import graphviz\n",
    "\n",
    "def render_dag_graph(dag):\n",
    "    check_cycle(dag)  # Ensure DAG has no cycles\n",
    "    dot = graphviz.Digraph(format='png')\n",
    "\n",
    "    for task in dag.tasks:\n",
    "        dot.node(task.task_id)\n",
    "        for downstream_task in task.downstream_list:\n",
    "            dot.edge(task.task_id, downstream_task.task_id)\n",
    "\n",
    "    dot.render(\"dag_visualization\")  # Saves as dag_visualization.png\n",
    "    return dot\n",
    "\n",
    "# Render and save DAG visualization\n",
    "render_dag_graph(dag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3ef1ba-59cb-4a4d-83cc-f0b34f8770a4",
   "metadata": {},
   "source": [
    "# Render ETL graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "054d0d3d-3799-4317-bb92-b221d925a7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAG visualization saved as dag_visualization.png\n"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph(format=\"png\")\n",
    "\n",
    "# Define ETL steps\n",
    "dot.node(\"Extract\", \"Extract Data\\n(API & Web Scraping)\", shape=\"box\", style=\"filled\", fillcolor=\"lightblue\")\n",
    "dot.node(\"Transform\", \"Transform Data\\n(NLP & Standardization)\", shape=\"box\", style=\"filled\", fillcolor=\"lightgreen\")\n",
    "dot.node(\"Load_Postgres\", \"Load to PostgreSQL\", shape=\"box\", style=\"filled\", fillcolor=\"lightyellow\")\n",
    "dot.node(\"Load_Elasticsearch\", \"Load to Elasticsearch\", shape=\"box\", style=\"filled\", fillcolor=\"lightyellow\")\n",
    "\n",
    "# Define connections\n",
    "dot.edge(\"Extract\", \"Transform\")\n",
    "dot.edge(\"Transform\", \"Load_Postgres\")\n",
    "dot.edge(\"Transform\", \"Load_Elasticsearch\")\n",
    "\n",
    "# Save as PNG\n",
    "dot.render(\"dag_visualization\")\n",
    "\n",
    "print(\"DAG visualization saved as dag_visualization.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18ccd35-c58f-42bb-9589-d5b1b8d623e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
